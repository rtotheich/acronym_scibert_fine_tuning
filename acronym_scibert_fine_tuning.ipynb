{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 22:22:47.577293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 22:22:49.147636: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/cuda/11.7/lib64:/shared/centos7/nodejs/14.15.4/lib\n",
      "2023-05-25 22:22:49.148458: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/cuda/11.7/lib64:/shared/centos7/nodejs/14.15.4/lib\n",
      "2023-05-25 22:22:49.148469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-25 22:22:53.226643: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 22:22:54.571525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 167 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0\n",
      "2023-05-25 22:23:05.075630: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 91.08MiB (rounded to 95508480)requested by op Mul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-05-25 22:23:05.075684: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2023-05-25 22:23:05.075699: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 5, Chunks in use: 5. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 40B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075711: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075722: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075733: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075742: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075752: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075762: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075771: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075781: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075791: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075800: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075810: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075820: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075829: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075839: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075848: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075858: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075868: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075879: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075891: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 167.12MiB allocated for chunks. 167.12MiB in use in bin. 91.08MiB client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075901: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 22:23:05.075912: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 91.08MiB was 64.00MiB, Chunk State: \n",
      "2023-05-25 22:23:05.075922: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 175243264\n",
      "2023-05-25 22:23:05.075935: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 2b481c000000 of size 256 next 1\n",
      "2023-05-25 22:23:05.075944: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 2b481c000100 of size 1280 next 2\n",
      "2023-05-25 22:23:05.075953: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 2b481c000600 of size 256 next 3\n",
      "2023-05-25 22:23:05.075961: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 2b481c000700 of size 256 next 4\n",
      "2023-05-25 22:23:05.075969: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 2b481c000800 of size 256 next 5\n",
      "2023-05-25 22:23:05.075977: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 2b481c000900 of size 256 next 6\n",
      "2023-05-25 22:23:05.075986: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 2b481c000a00 of size 175240704 next 18446744073709551615\n",
      "2023-05-25 22:23:05.075995: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-05-25 22:23:05.076005: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 256 totalling 1.2KiB\n",
      "2023-05-25 22:23:05.076014: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-05-25 22:23:05.076023: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 175240704 totalling 167.12MiB\n",
      "2023-05-25 22:23:05.076033: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 167.12MiB\n",
      "2023-05-25 22:23:05.076041: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 175243264 memory_limit_: 175243264 available bytes: 0 curr_region_allocation_bytes_: 350486528\n",
      "2023-05-25 22:23:05.076054: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                       175243264\n",
      "InUse:                       175243264\n",
      "MaxInUse:                    175243264\n",
      "NumAllocs:                           7\n",
      "MaxAllocSize:                175240704\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-05-25 22:23:05.076067: W tensorflow/tsl/framework/bfc_allocator.cc:492] *******************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "2023-05-25 22:23:05.076116: W tensorflow/core/framework/op_kernel.cc:1818] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n\n{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]\n\nCall arguments received by layer 'bert' (type TFBertMainLayer):\n  • input_ids=tf.Tensor(shape=(3, 5), dtype=int32)\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=False\n  • output_hidden_states=False\n  • return_dict=True\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13151/2764855081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"allenai/scibert_scivocab_uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             return model_class.from_pretrained(\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             )\n\u001b[1;32m    470\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m                 \u001b[0moutput_loading_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loading_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2801\u001b[0m                 \u001b[0m_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_weight_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2802\u001b[0;31m                 \u001b[0mtf_to_pt_weight_rename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_to_pt_weight_rename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2803\u001b[0m             )\n\u001b[1;32m   2804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/modeling_tf_pytorch_utils.py\u001b[0m in \u001b[0;36mload_pytorch_checkpoint_in_tf2_model\u001b[0;34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0moutput_loading_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loading_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0m_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mtf_to_pt_weight_rename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_to_pt_weight_rename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/modeling_tf_pytorch_utils.py\u001b[0m in \u001b[0;36mload_pytorch_weights_in_tf2_model\u001b[0;34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0moutput_loading_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loading_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0m_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtf_to_pt_weight_rename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_to_pt_weight_rename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/modeling_tf_pytorch_utils.py\u001b[0m in \u001b[0;36mload_pytorch_state_dict_in_tf2_model\u001b[0;34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mtf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make sure model is built\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;31m# Adapt state dict - TODO remove this and update the AWS weights files instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# Convert old format to new format if needed from a PyTorch state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m         )\n\u001b[1;32m   1346\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/tf_v2/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n\n{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]\n\nCall arguments received by layer 'bert' (type TFBertMainLayer):\n  • input_ids=tf.Tensor(shape=(3, 5), dtype=int32)\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=False\n  • output_hidden_states=False\n  • return_dict=True\n  • training=False"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"allenai/scibert_scivocab_uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint, from_pt=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/yue.r/.cache/huggingface/datasets/csv/default-6309002f5eafbfbd/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af87263e6a042e0b4bab85b9985ee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ec7319a6254c37895a7a496413f91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/yue.r/.cache/huggingface/datasets/csv/default-6309002f5eafbfbd/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5fa7a4b9b3478ba39cdba75a63117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['masked', 'outputs'],\n",
       "        num_rows: 504\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked', 'outputs'],\n",
       "        num_rows: 168\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\":\"acronym_train.csv\", \"test\":\"acronym_test.csv\"})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'masked': 'femoral artery ([MASK])', 'outputs': 'femoral artery (FA)'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/yue.r/.cache/huggingface/datasets/csv/default-863b486ab570f9ce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-011f33912d36de24.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Input IDs: transcription factor ([MASK])\n",
      "'>>> Outputs: transcription factor (TF)\n",
      "'>>> Input IDs: minimal inhibitory concentration ([MASK])\n",
      "'>>> Outputs: minimal inhibitory concentration (MIC)\n",
      "'>>> Input IDs: high level cephalosporinase ([MASK])\n",
      "'>>> Outputs: high level cephalosporinase (HL-CASE)\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"'>>> Input IDs: {row['masked']}\")\n",
    "    print(f\"'>>> Outputs: {row['outputs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"masked\"], padding='max_length', max_length=30)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def tokenize_labels(examples):\n",
    "    result = tokenizer(examples[\"outputs\"], padding='max_length', max_length=30)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yue.r/.cache/huggingface/datasets/csv/default-863b486ab570f9ce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-468df5f81e0d0012.arrow\n",
      "Loading cached processed dataset at /home/yue.r/.cache/huggingface/datasets/csv/default-863b486ab570f9ce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b2be5607922cfa08.arrow\n",
      "Loading cached processed dataset at /home/yue.r/.cache/huggingface/datasets/csv/default-863b486ab570f9ce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f5f778dab4111420.arrow\n",
      "Loading cached processed dataset at /home/yue.r/.cache/huggingface/datasets/csv/default-863b486ab570f9ce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0010c80a190cd5b3.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 504\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 168\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_labels, batched=True, remove_columns=[\"outputs\"]\n",
    ")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"word_ids\", \"masked\", \"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    ")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yue.r/miniconda/envs/tf_v2/lib/python3.7/site-packages/datasets/arrow_dataset.py:396: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 30), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(None, 30), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 30), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 30), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tf_train = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle = True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_test = tokenized_dataset[\"test\"].to_tf_dataset(\n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle = True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100-SXM2-32GB, compute capability 7.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01\n",
    ")\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "16/16 [==============================] - 21s 123ms/step - loss: 13.0101 - accuracy: 0.1733\n",
      "Epoch 2/25\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 9.4692 - accuracy: 0.1748\n",
      "Epoch 3/25\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 2.8866 - accuracy: 0.6587\n",
      "Epoch 4/25\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 1.4584 - accuracy: 0.8485\n",
      "Epoch 5/25\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.8486 - accuracy: 0.8773\n",
      "Epoch 6/25\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.4981 - accuracy: 0.9063\n",
      "Epoch 7/25\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.3330 - accuracy: 0.9290\n",
      "Epoch 8/25\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.2713 - accuracy: 0.9318\n",
      "Epoch 9/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.2340 - accuracy: 0.9354\n",
      "Epoch 10/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.2110 - accuracy: 0.9386\n",
      "Epoch 11/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1971 - accuracy: 0.9392\n",
      "Epoch 12/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1842 - accuracy: 0.9423\n",
      "Epoch 13/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1728 - accuracy: 0.9435\n",
      "Epoch 14/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1601 - accuracy: 0.9488\n",
      "Epoch 15/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1480 - accuracy: 0.9515\n",
      "Epoch 16/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1366 - accuracy: 0.9542\n",
      "Epoch 17/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1244 - accuracy: 0.9592\n",
      "Epoch 18/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1201 - accuracy: 0.9589\n",
      "Epoch 19/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1107 - accuracy: 0.9630\n",
      "Epoch 20/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1017 - accuracy: 0.9644\n",
      "Epoch 21/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.0915 - accuracy: 0.9687\n",
      "Epoch 22/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.0914 - accuracy: 0.9690\n",
      "Epoch 23/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.0786 - accuracy: 0.9734\n",
      "Epoch 24/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.0722 - accuracy: 0.9755\n",
      "Epoch 25/25\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.0674 - accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aae06a1e1d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_length(lf):\n",
    "    count = 0\n",
    "    for word in lf.split():\n",
    "        if (\"(\" not in word and\n",
    "            \")\" not in word and\n",
    "            word != \"and\" and\n",
    "            word != \"or\" and\n",
    "            word != \"of\"):\n",
    "            if \"-\" in word and word[-1] != \"- \":\n",
    "                count += 1\n",
    "            count +=1\n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(lf):\n",
    "    prediction = None\n",
    "    inputs = tokenizer(lf, return_tensors=\"np\")\n",
    "    token_logits = model(**inputs).logits\n",
    "    \n",
    "    mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "    top_token_index_1 = np.argsort(-mask_token_logits)[0]\n",
    "    \n",
    "    lf = lf.replace(tokenizer.mask_token, tokenizer.decode(top_token_index_1).upper())\n",
    "    \n",
    "    phrase_length = get_length(lf)\n",
    "    \n",
    "    if len(tokenizer.decode(top_token_index_1)) < phrase_length:\n",
    "        lf = lf.replace(\")\", \" \" + tokenizer.mask_token + \")\")\n",
    "        inputs = tokenizer(lf, return_tensors=\"np\")\n",
    "        token_logits = model(**inputs).logits\n",
    "        mask_token_index2 = mask_token_index + 1\n",
    "        mask_token_logits2 = token_logits[0, mask_token_index2, :]\n",
    "        top_token_index_2 = np.argsort(-mask_token_logits2)[0]\n",
    "        prediction = lf.replace(\" \" + tokenizer.mask_token, tokenizer.decode(top_token_index_2)[2:].upper())\n",
    "        \n",
    "    if prediction == None:\n",
    "        prediction = lf\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(dataset[\"test\"])):\n",
    "    results.append(predict(dataset[\"test\"][i][\"masked\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "fields = [\"prediction\", \"gold_labels\"]\n",
    "\n",
    "gold_labels = dataset[\"test\"][\"outputs\"]\n",
    "\n",
    "data = zip(results, gold_labels)\n",
    "\n",
    "with open('predictions.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for item in data:\n",
    "        writer.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:09:17.727220: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 626774400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 168\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = model.predict(tf_test)\n",
    "tokenized_dataset[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
